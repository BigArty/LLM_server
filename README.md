# LLM_server
Simple code for fine-grained control over local LLM inference in openai-like API

To run the server use python server.py

All settings are in the code, so you need to change them according to your liking.